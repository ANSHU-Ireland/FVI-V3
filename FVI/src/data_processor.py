"""
FVI Data Processing Module
Handles ingestion, cleaning, and transformation of all datasets for the 5 target countries.
"""

import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import yaml
import logging
from datetime import datetime

class FVIDataProcessor:
    def __init__(self, config_path: str, data_sources_path: str):
        """Initialize the data processor with configuration and data sources."""
        self.config_path = config_path
        self.data_sources_path = Path(data_sources_path)
        self.workspace_path = Path(config_path).parent.parent
        
        # Load configuration
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.target_countries = self.config['countries']  # [IND, CHN, USA, JPN, ZAF]
        self.reference_year = None
        self.dataset_registry = {}
        
        # Setup logging
        self._setup_logging()
        
        # Country name mappings for different datasets
        self.country_mappings = {
            'China': 'CHN',
            'United States': 'USA',
            'India': 'IND', 
            'Japan': 'JPN',
            'South Africa': 'ZAF',
            'US': 'USA',
            'United States of America': 'USA'
        }
    
    def _setup_logging(self):
        """Setup logging for the data processor."""
        log_dir = self.workspace_path / "artifacts" / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"data_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def determine_reference_year(self) -> int:
        """
        Determine reference year based on config policy: latest_year_present_in_>=80_percent
        """
        self.logger.info("Determining reference year...")
        
        # Key time-series files to check
        time_series_files = {
            'emissions': 'Emission/annual-co2-emission.csv',
            'coal_consumption': 'economic sources/Coal Consumption by Countries (Year Wise).xlsx',
            'electricity_coal': 'economic sources/Electricity generated by coal.xlsx'
        }
        
        available_years_by_file = {}
        
        # Check emissions file
        emissions_path = self.data_sources_path / time_series_files['emissions']
        if emissions_path.exists():
            df = pd.read_csv(emissions_path)
            # Filter to target countries first
            target_df = df[df['Code'].isin(self.target_countries)]
            available_years_by_file['emissions'] = set(target_df['Year'].dropna().astype(int))
        
        # Check coal consumption file
        coal_consumption_path = self.data_sources_path / time_series_files['coal_consumption']
        if coal_consumption_path.exists():
            df = pd.read_excel(coal_consumption_path, sheet_name='coal-consumption-by-country-ter')
            # Map entity names to ISO3
            df['iso3'] = df['Entity'].map(self.country_mappings)
            target_df = df[df['iso3'].isin(self.target_countries)]
            available_years_by_file['coal_consumption'] = set(target_df['Year'].dropna().astype(int))
        
        # Check electricity from coal file
        electricity_path = self.data_sources_path / time_series_files['electricity_coal']
        if electricity_path.exists():
            df = pd.read_excel(electricity_path)
            # Filter for coal electricity series
            coal_series = df[df['Series Name'].str.contains('coal', case=False, na=False)]
            if not coal_series.empty:
                # Get year columns (should be numeric)
                year_cols = [col for col in coal_series.columns if str(col).isdigit()]
                available_years_by_file['electricity_coal'] = set([int(col) for col in year_cols])
        
        if not available_years_by_file:
            self.logger.warning("No time series files found, defaulting to 2022")
            self.reference_year = 2022
            return self.reference_year
        
        # Find intersection of years
        all_years = set.intersection(*available_years_by_file.values()) if available_years_by_file else set()
        
        if all_years:
            self.reference_year = max(all_years)
            self.logger.info(f"Reference year from intersection: {self.reference_year}")
        else:
            # Find latest year present in >=80% of files
            all_possible_years = set.union(*available_years_by_file.values())
            threshold = len(available_years_by_file) * 0.8
            
            valid_years = []
            for year in all_possible_years:
                count = sum(1 for years in available_years_by_file.values() if year in years)
                if count >= threshold:
                    valid_years.append(year)
            
            if valid_years:
                self.reference_year = max(valid_years)
                self.logger.info(f"Reference year from 80% threshold: {self.reference_year}")
            else:
                # Fallback to most recent year
                self.reference_year = max(all_possible_years) if all_possible_years else 2022
                self.logger.warning(f"Fallback reference year: {self.reference_year}")
        
        # Save reference year to metadata
        meta_path = self.workspace_path / "artifacts" / "logs" / "run_meta.json"
        meta_path.parent.mkdir(parents=True, exist_ok=True)
        
        meta_data = {
            'reference_year': self.reference_year,
            'available_years_by_file': {k: list(v) for k, v in available_years_by_file.items()},
            'determined_at': datetime.now().isoformat()
        }
        
        with open(meta_path, 'w') as f:
            json.dump(meta_data, f, indent=2)
        
        return self.reference_year
    
    def clean_emissions_data(self) -> pd.DataFrame:
        """Clean annual CO2 emissions data."""
        self.logger.info("Cleaning emissions data...")
        
        file_path = self.data_sources_path / "Emission/annual-co2-emission.csv"
        if not file_path.exists():
            self.logger.error(f"Emissions file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path)
        
        # Filter to target countries
        df_filtered = df[df['Code'].isin(self.target_countries)].copy()
        
        # Rename columns
        df_filtered = df_filtered.rename(columns={
            'Code': 'iso3',
            'Year': 'year',
            'Annual COâ‚‚ emissions from coal': 'emissions_raw'
        })
        
        # Convert units (assuming Mt to tonnes)
        df_filtered['emissions_tco2e'] = df_filtered['emissions_raw'] * 1e6
        
        # Filter to reference year or latest available
        df_filtered['data_quality'] = 1.0
        result_data = []
        
        for country in self.target_countries:
            country_data = df_filtered[df_filtered['iso3'] == country]
            if country_data.empty:
                continue
            
            # Try to get reference year data
            ref_year_data = country_data[country_data['year'] == self.reference_year]
            if not ref_year_data.empty:
                row = ref_year_data.iloc[0].copy()
            else:
                # Get latest available year <= reference year
                available_data = country_data[country_data['year'] <= self.reference_year]
                if not available_data.empty:
                    row = available_data.loc[available_data['year'].idxmax()].copy()
                    row['data_quality'] = 0.8  # Quality penalty for fallback
                else:
                    continue
            
            row['dataset_ids'] = ['annual-co2-emission.csv']
            result_data.append(row)
        
        result_df = pd.DataFrame(result_data) if result_data else pd.DataFrame()
        self.logger.info(f"Emissions data cleaned: {len(result_df)} records")
        return result_df
    
    def clean_coal_consumption_data(self) -> pd.DataFrame:
        """Clean coal consumption data."""
        self.logger.info("Cleaning coal consumption data...")
        
        file_path = self.data_sources_path / "economic sources/Coal Consumption by Countries (Year Wise).xlsx"
        if not file_path.exists():
            self.logger.error(f"Coal consumption file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_excel(file_path, sheet_name='coal-consumption-by-country-ter')
        
        # Map entity names to ISO3
        df['iso3'] = df['Entity'].map(self.country_mappings)
        df_filtered = df[df['iso3'].isin(self.target_countries)].copy()
        
        # Rename columns
        df_filtered = df_filtered.rename(columns={
            'Year': 'year',
            'Coal consumption - TWh': 'coal_consumption_twh'
        })
        
        # Filter to reference year or latest available
        df_filtered['data_quality'] = 1.0
        result_data = []
        
        for country in self.target_countries:
            country_data = df_filtered[df_filtered['iso3'] == country]
            if country_data.empty:
                continue
            
            ref_year_data = country_data[country_data['year'] == self.reference_year]
            if not ref_year_data.empty:
                row = ref_year_data.iloc[0].copy()
            else:
                available_data = country_data[country_data['year'] <= self.reference_year]
                if not available_data.empty:
                    row = available_data.loc[available_data['year'].idxmax()].copy()
                    row['data_quality'] = 0.8
                else:
                    continue
            
            row['dataset_ids'] = ['Coal Consumption by Countries (Year Wise).xlsx']
            result_data.append(row)
        
        result_df = pd.DataFrame(result_data) if result_data else pd.DataFrame()
        self.logger.info(f"Coal consumption data cleaned: {len(result_df)} records")
        return result_df
    
    def clean_electricity_coal_data(self) -> pd.DataFrame:
        """Clean electricity from coal percentage data."""
        self.logger.info("Cleaning electricity from coal data...")
        
        file_path = self.data_sources_path / "economic sources/Electricity generated by coal.xlsx"
        if not file_path.exists():
            self.logger.error(f"Electricity coal file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_excel(file_path)
        
        # Filter for coal electricity series
        coal_series = df[df['Series Name'].str.contains('coal', case=False, na=False)]
        if coal_series.empty:
            self.logger.warning("No coal electricity series found")
            return pd.DataFrame()
        
        # Get year columns
        year_cols = [col for col in coal_series.columns if str(col).isdigit()]
        
        # Melt to long format
        id_cols = ['Country Name', 'Country Code', 'Series Name']
        melted = coal_series.melt(id_vars=id_cols, value_vars=year_cols, 
                                 var_name='year', value_name='coal_share_electricity_pct')
        
        melted['year'] = melted['year'].astype(int)
        melted = melted.rename(columns={'Country Code': 'iso3'})
        
        # Filter to target countries
        df_filtered = melted[melted['iso3'].isin(self.target_countries)].copy()
        
        # Clamp to 0-100
        df_filtered['coal_share_electricity_pct'] = df_filtered['coal_share_electricity_pct'].clip(0, 100)
        
        # Filter to reference year or latest available
        df_filtered['data_quality'] = 1.0
        result_data = []
        
        for country in self.target_countries:
            country_data = df_filtered[df_filtered['iso3'] == country]
            if country_data.empty:
                continue
            
            ref_year_data = country_data[country_data['year'] == self.reference_year]
            if not ref_year_data.empty:
                row = ref_year_data.iloc[0].copy()
            else:
                available_data = country_data[country_data['year'] <= self.reference_year]
                if not available_data.empty:
                    row = available_data.loc[available_data['year'].idxmax()].copy()
                    row['data_quality'] = 0.8
                else:
                    continue
            
            row['dataset_ids'] = ['Electricity generated by coal.xlsx']
            result_data.append(row)
        
        result_df = pd.DataFrame(result_data) if result_data else pd.DataFrame()
        self.logger.info(f"Electricity coal data cleaned: {len(result_df)} records")
        return result_df
    
    def clean_mining_area_data(self) -> pd.DataFrame:
        """Clean global mining area data."""
        self.logger.info("Cleaning mining area data...")
        
        file_path = self.data_sources_path / "Ecological Data/global_mining_area_per_country_v1.csv"
        if not file_path.exists():
            self.logger.error(f"Mining area file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path)
        
        # Filter to target countries
        df_filtered = df[df['ISO3_CODE'].isin(self.target_countries)].copy()
        
        # Rename columns
        df_filtered = df_filtered.rename(columns={
            'ISO3_CODE': 'iso3',
            'AREA': 'mining_area_km2',
            'N_FEATURES': 'mining_sites_count'
        })
        
        df_filtered['data_quality'] = 1.0
        df_filtered['dataset_ids'] = [['global_mining_area_per_country_v1.csv']] * len(df_filtered)
        
        self.logger.info(f"Mining area data cleaned: {len(df_filtered)} records")
        return df_filtered
    
    def clean_deforestation_data(self) -> pd.DataFrame:
        """Clean deforestation fronts data."""
        self.logger.info("Cleaning deforestation data...")
        
        file_path = self.data_sources_path / "Ecological Data/New_Deforestation_Fronts_wgs[1].csv"
        if not file_path.exists():
            self.logger.error(f"Deforestation file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path)
        
        # Load mapping from template
        mapping_path = self.workspace_path / "data/templates/deforestation_fronts_country.csv"
        if mapping_path.exists():
            mapping_df = pd.read_csv(mapping_path)
            # Join with deforestation data
            df_mapped = df.merge(mapping_df, left_on='Name', right_on='front_name', how='inner')
            df_mapped = df_mapped[df_mapped['iso3'].isin(self.target_countries)]
            
            # Convert hectares to km2 and aggregate by country
            df_mapped['area_km2'] = df_mapped['Area_ha'] / 100
            result = df_mapped.groupby('iso3')['area_km2'].sum().reset_index()
            result = result.rename(columns={'area_km2': 'deforest_area_km2'})
            result['data_quality'] = 1.0
            result['dataset_ids'] = [['New_Deforestation_Fronts_wgs[1].csv']] * len(result)
        else:
            self.logger.warning("Deforestation mapping template not found, skipping")
            result = pd.DataFrame()
        
        self.logger.info(f"Deforestation data cleaned: {len(result)} records")
        return result
    
    def clean_coal_phase_out_data(self) -> pd.DataFrame:
        """Clean coal phase-out timeline data."""
        self.logger.info("Cleaning coal phase-out data...")
        
        file_path = self.data_sources_path / "Artificial Support Score/coal-phase-out-timeline.csv"
        if not file_path.exists():
            self.logger.error(f"Coal phase-out file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path)
        
        # Filter to target countries
        df_filtered = df[df['Code'].isin(self.target_countries)].copy()
        
        # Map timeline to exit year
        def map_exit_year(timeline):
            if pd.isna(timeline) or 'No commitment' in str(timeline):
                return 2100
            elif '2020s' in str(timeline):
                return 2025
            elif '2030s' in str(timeline):
                return 2035
            elif '2040s' in str(timeline):
                return 2045
            elif str(timeline).isdigit():
                return int(timeline)
            else:
                return 2100
        
        df_filtered['exit_year'] = df_filtered['Coal Exit Timeline (Beyond Coal)'].apply(map_exit_year)
        df_filtered = df_filtered.rename(columns={'Code': 'iso3'})
        df_filtered['year'] = self.reference_year
        df_filtered['data_quality'] = 1.0
        df_filtered['dataset_ids'] = [['coal-phase-out-timeline.csv']] * len(df_filtered)
        
        self.logger.info(f"Coal phase-out data cleaned: {len(df_filtered)} records")
        return df_filtered
    
    def clean_ownership_data(self) -> pd.DataFrame:
        """Clean coal mining ownership data."""
        self.logger.info("Cleaning ownership data...")
        
        file_path = self.data_sources_path / "Artificial Support Score/coal-mining_emissions_sources_ownership_v4_4_0.csv"
        if not file_path.exists():
            self.logger.error(f"Ownership file not found: {file_path}")
            return pd.DataFrame()
        
        df = pd.read_csv(file_path)
        
        # Find the ISO3 column (might have different names)
        iso3_col = None
        for col in df.columns:
            if 'iso3' in col.lower() or 'country' in col.lower():
                iso3_col = col
                break
        
        if iso3_col is None:
            self.logger.error("No ISO3 column found in ownership data")
            return pd.DataFrame()
        
        # Filter to target countries
        df_filtered = df[df[iso3_col].isin(self.target_countries)].copy()
        
        # Calculate state ownership percentage
        result_data = []
        for country in self.target_countries:
            country_data = df_filtered[df_filtered[iso3_col] == country]
            if country_data.empty:
                continue
            
            # Sum state and mixed state ownership
            state_cols = ['parent_entity_type', 'overall_share_percent']
            if all(col in country_data.columns for col in state_cols):
                state_ownership = country_data[
                    country_data['parent_entity_type'].isin(['state', 'mixed_state'])
                ]['overall_share_percent'].sum()
                
                total_ownership = country_data['overall_share_percent'].sum()
                
                state_share_pct = (state_ownership / total_ownership * 100) if total_ownership > 0 else 0
            else:
                state_share_pct = 0
            
            result_data.append({
                'iso3': country,
                'year': self.reference_year,
                'state_ownership_share_pct': state_share_pct,
                'data_quality': 1.0,
                'dataset_ids': ['coal-mining_emissions_sources_ownership_v4_4_0.csv']
            })
        
        result_df = pd.DataFrame(result_data)
        self.logger.info(f"Ownership data cleaned: {len(result_df)} records")
        return result_df
    
    def load_micro_tables(self) -> Dict[str, pd.DataFrame]:
        """Load micro-tables (coverage, deforestation mapping, ash sites)."""
        self.logger.info("Loading micro-tables...")
        
        micro_tables = {}
        templates_dir = self.workspace_path / "data/templates"
        
        # Coverage data
        coverage_path = templates_dir / "coverage_country.csv"
        if coverage_path.exists():
            micro_tables['coverage'] = pd.read_csv(coverage_path)
        
        # Deforestation mapping
        deforest_path = templates_dir / "deforestation_fronts_country.csv"
        if deforest_path.exists():
            micro_tables['deforestation_mapping'] = pd.read_csv(deforest_path)
        
        # Ash sites
        ash_path = templates_dir / "coal_ash_sites_by_country.csv"
        if ash_path.exists():
            micro_tables['ash_sites'] = pd.read_csv(ash_path)
        
        return micro_tables
    
    def process_all_data(self) -> Dict[str, pd.DataFrame]:
        """Process all datasets and return cleaned fact tables."""
        self.logger.info("Starting data processing pipeline...")
        
        # Determine reference year first
        self.determine_reference_year()
        
        # Process each dataset
        datasets = {
            'emissions': self.clean_emissions_data(),
            'coal_consumption': self.clean_coal_consumption_data(),
            'electricity_coal': self.clean_electricity_coal_data(),
            'mining_area': self.clean_mining_area_data(),
            'deforestation': self.clean_deforestation_data(),
            'phase_out': self.clean_coal_phase_out_data(),
            'ownership': self.clean_ownership_data()
        }
        
        # Load micro-tables
        micro_tables = self.load_micro_tables()
        datasets.update(micro_tables)
        
        # Create canonical fact tables
        fact_tables = self._create_fact_tables(datasets)
        
        # Save cleaned data
        self._save_cleaned_data(fact_tables)
        
        self.logger.info("Data processing pipeline completed")
        return fact_tables
    
    def _create_fact_tables(self, datasets: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """Create canonical fact tables from cleaned datasets."""
        
        # Country dimension
        country_names = {
            'IND': 'India',
            'CHN': 'China', 
            'USA': 'United States',
            'JPN': 'Japan',
            'ZAF': 'South Africa'
        }
        
        country_dim = pd.DataFrame([
            {'iso3': iso3, 'name': name} for iso3, name in country_names.items()
        ])
        
        # Fact emissions
        fact_emissions = datasets['emissions'][['iso3', 'year', 'emissions_tco2e', 'data_quality', 'dataset_ids']].copy()
        if 'coverage' in datasets and not datasets['coverage'].empty:
            fact_emissions = fact_emissions.merge(
                datasets['coverage'][['iso3', 'year', 'coverage_pct']], 
                on=['iso3', 'year'], 
                how='left'
            )
        
        # Fact necessity
        necessity_data = []
        if not datasets['electricity_coal'].empty and not datasets['coal_consumption'].empty:
            elec_data = datasets['electricity_coal'][['iso3', 'year', 'coal_share_electricity_pct', 'data_quality']].copy()
            cons_data = datasets['coal_consumption'][['iso3', 'year', 'coal_consumption_twh', 'data_quality']].copy()
            
            fact_necessity = elec_data.merge(cons_data, on=['iso3', 'year'], how='outer', suffixes=('_elec', '_cons'))
            fact_necessity['data_quality'] = fact_necessity[['data_quality_elec', 'data_quality_cons']].min(axis=1)
            fact_necessity['dataset_ids'] = [['Electricity generated by coal.xlsx', 'Coal Consumption by Countries (Year Wise).xlsx']] * len(fact_necessity)
            fact_necessity = fact_necessity[['iso3', 'year', 'coal_share_electricity_pct', 'coal_consumption_twh', 'data_quality', 'dataset_ids']]
        else:
            fact_necessity = pd.DataFrame()
        
        # Fact ecological
        fact_ecological = datasets['mining_area'][['iso3', 'mining_area_km2', 'mining_sites_count', 'data_quality', 'dataset_ids']].copy()
        if not datasets['deforestation'].empty:
            fact_ecological = fact_ecological.merge(
                datasets['deforestation'][['iso3', 'deforest_area_km2']], 
                on='iso3', 
                how='left'
            )
        
        # Add activity denominator (coal consumption in GWh)
        if not datasets['coal_consumption'].empty:
            activity_data = datasets['coal_consumption'][['iso3', 'coal_consumption_twh']].copy()
            activity_data['activity_denominator_value'] = activity_data['coal_consumption_twh'] * 1000  # TWh to GWh
            activity_data['activity_denominator_unit'] = 'GWh'
            
            fact_ecological = fact_ecological.merge(
                activity_data[['iso3', 'activity_denominator_value', 'activity_denominator_unit']], 
                on='iso3', 
                how='left'
            )
        
        # Fact support
        fact_support = datasets['phase_out'][['iso3', 'year', 'exit_year', 'data_quality', 'dataset_ids']].copy()
        if not datasets['ownership'].empty:
            fact_support = fact_support.merge(
                datasets['ownership'][['iso3', 'state_ownership_share_pct']], 
                on='iso3', 
                how='left'
            )
        
        return {
            'country_dim': country_dim,
            'fact_emissions': fact_emissions,
            'fact_necessity': fact_necessity,
            'fact_ecological': fact_ecological,
            'fact_support': fact_support
        }
    
    def _save_cleaned_data(self, fact_tables: Dict[str, pd.DataFrame]):
        """Save cleaned fact tables to data/clean/."""
        clean_dir = self.workspace_path / "data/clean"
        clean_dir.mkdir(parents=True, exist_ok=True)
        
        for table_name, df in fact_tables.items():
            if not df.empty:
                df.to_csv(clean_dir / f"{table_name}.csv", index=False)
                self.logger.info(f"Saved {table_name}: {len(df)} records")
